import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel
from torch.optim import AdamW
import numpy as np
import matplotlib.pyplot as plt
import os
import json
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import precision_recall_curve, roc_curve, precision_score, recall_score,classification_report, f1_score, accuracy_score
import utils
from data_loader import load_train_data, load_test_data
import metrics
from sklearn.metrics import confusion_matrix
import pandas as pd
import math
from preprocess_testdata import load_and_preprocess_test



# ------------------ Dataset & Model ------------------
class TimeSeriesDataset(Dataset):
    def __init__(self, sequences, targets):
        self.sequences = sequences
        self.targets = targets

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return {
            'inputs': torch.tensor(self.sequences[idx], dtype=torch.float32),
            'targets': torch.tensor(self.targets[idx], dtype=torch.float32)
        }

class TimeSeriesBERTFineTune(nn.Module):
    def __init__(
        self,
        input_dim=39,
        hidden_size=768,
        seq_len=50,
        output_dim=39,
        freeze_until_layer=12       
    ):
        super().__init__()
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # (a) projection
        self.embedding = nn.Linear(input_dim, hidden_size)

        # (b) sinusoidal PE ƒë∆∞·ª£c l∆∞u d∆∞·ªõi buffer
        pe = self.sinusoidal_positional_embedding(seq_len + 1, hidden_size)
        self.register_buffer("pos_embedding", pe)     # kh√¥ng r∆°i v√†o .parameters()

        # (c) learnable CLS, kh·ªüi t·∫°o truncated‚Äënormal std=0.02
        self.cls_token = nn.Parameter(torch.empty(1, 1, hidden_size))
        nn.init.trunc_normal_(self.cls_token, std=0.02)

        # (d) load SecBERT
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        

        # (e) freeze ph·∫ßn d∆∞·ªõi
        # for layer in self.bert.encoder.layer[:freeze_until_layer]:
        #     for p in layer.parameters():
        #         p.requires_grad = False

        # (f) head
        self.output_layer = nn.Linear(hidden_size, output_dim)
        self.to(self.device)

    def sinusoidal_positional_embedding(self, seq_len, hidden_dim):
        position = torch.arange(0, seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, hidden_dim, 2) * (-math.log(10000.0) / hidden_dim))
        pe = torch.zeros(seq_len, hidden_dim)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)

    def forward(self, x):
        """
        x shape: (B, seq_len, input_dim)
        Return: (B, output_dim)
        """
        batch_size = x.size(0)

        # 1) Embedding
        x = self.embedding(x)  # (B, seq_len, hidden_size)

        # 2) Th√™m [CLS] v√†o ƒë·∫ßu
        cls_expanded = self.cls_token.expand(batch_size, 1, -1)  # (B, 1, hidden_size)
        x = torch.cat([cls_expanded, x], dim=1)  # (B, seq_len + 1, hidden_size)

        # 3) Th√™m positional embedding
        # pos_embedding shape: (1, seq_len+1, hidden_size)
        # x shape: (B, seq_len+1, hidden_size)
        x = x + self.pos_embedding[:, : x.size(1)] 

        # 4) Attention mask
        attention_mask = torch.ones(x.size(0), x.size(1), dtype=torch.long, device=self.device)

        # 5) Forward qua BERT
        outputs = self.bert(inputs_embeds=x, attention_mask=attention_mask)

        # 6) L·∫•y hidden state c·ªßa [CLS] => index 0
        cls_state = outputs.last_hidden_state[:, 0, :]  # (B, hidden_size)

        # 7) Qua output_layer
        prediction = self.output_layer(cls_state)  # (B, output_dim)

        return prediction
    def compute_loss(self, predictions, targets, lambda_smooth=0.1):
        mse_loss = nn.MSELoss()(predictions, targets)
        # Smoothness regularization (differences between consecutive predictions)
        loss_smoothness = torch.mean((predictions[:, 1:] - predictions[:, :-1]) ** 2)
        total_loss = mse_loss + lambda_smooth * loss_smoothness
        return total_loss
    @torch.no_grad()
    def reconstruction_errors(self, X, history=100, device='cuda', batch_size=512):
        """
        T√≠nh reconstruction error theo ki·ªÉu sliding window:
          - T∆∞∆°ng t·ª± logic 'sequence' b·∫°n ƒë√£ l√†m, i.e. X[i-history:i] -> d·ª± ƒëo√°n X[i]
          - Tr·∫£ v·ªÅ m·∫£ng (num_samples, output_dim) ch·ª©a MSE per-feature ·ªü m·ªói timestep.
        
        X: np.array shape (N, input_dim) 
           => N ph·∫£i >= history
        Return: errors shape (N - history, output_dim),
                m·ªói d√≤ng l√† (pred - ground_truth)^2 per-feature
        """
        self.eval()  # model eval
        all_errors = []

        # Duy·ªát theo batch ƒë·ªÉ tr√°nh OOM
        num_samples = len(X)
        start_idx = history+1
        end_idx   = num_samples

        # Ch·∫°y v√≤ng l·∫∑p chunk
        idx = start_idx
        while idx < end_idx:
            # L·∫•y batch t·ª´ idx -> idx+batch_size
            batch_end = min(idx + batch_size, end_idx)
            # L·∫•y input sequences
            seq_list = []
            for i in range(idx, batch_end):
                seq = X[i-history:i]  # shape (history, input_dim)
                seq_list.append(seq)
            seq_batch = np.stack(seq_list, axis=0)  # (batch_size, history, input_dim)

            seq_batch_torch = torch.tensor(seq_batch, dtype=torch.float32, device=device)
            preds = self.forward(seq_batch_torch)  # (batch_size, output_dim)

            # ground truth = X[idx->batch_end], shape (batch_size, input_dim)
            # so s√°nh => error
            ground_truth = X[idx:batch_end]  # (batch_size, input_dim)
            ground_truth_torch = torch.tensor(ground_truth, dtype=torch.float32, device=device)

            # MSE m·ªói feature
            # shape (batch_size, output_dim)
            squared_error = (preds - ground_truth_torch) ** 2

            all_errors.append(squared_error.cpu().numpy())
            idx = batch_end

        # gh√©p t·∫•t c·∫£ batch l·∫°i
        all_errors = np.concatenate(all_errors, axis=0)  # shape ((N-history), output_dim)
        return all_errors
    def reconstruction_errors_by_idxs(self, Xfull, idxs, history, bs=128, device='cuda'):
        self.eval()
        full_errors = np.zeros((len(idxs), Xfull.shape[1]))  # (num_samples, num_features)

        for start in range(0, len(idxs), bs):
            end = min(start + bs, len(idxs))
            batch_idxs = idxs[start:end]

            # Chu·∫©n b·ªã batch input v√† target
            Xbatch = np.array([Xfull[i - history:i] for i in batch_idxs])  # shape (B, history, input_dim)
            Ybatch = np.array([Xfull[i+1] for i in batch_idxs])              # shape (B, input_dim)

            # ƒê∆∞a v√†o PyTorch
            X_tensor = torch.tensor(Xbatch, dtype=torch.float32).to(device)
            Y_tensor = torch.tensor(Ybatch, dtype=torch.float32).to(device)

            # D·ª± ƒëo√°n
            preds = self.forward(X_tensor)  # self l√† model

            # T√≠nh l·ªói b√¨nh ph∆∞∆°ng
            squared_error = (preds - Y_tensor) ** 2  # shape (B, output_dim)

            # G√°n v√†o full_errors
            full_errors[start:end] = squared_error.detach().cpu().numpy()


        return full_errors
    def cached_detect(self, instance_errors, theta, window=1):
        """
        instance_errors: shape (num_samples, output_dim),
                         th∆∞·ªùng l√† KQ c·ªßa reconstruction_errors(...)

        - T√≠nh trung b√¨nh l·ªói tr√™n c√°c feature => instance_errors.mean(axis=1) => 1-D
        - So s√°nh v·ªõi threshold => 0/1
        - √Åp d·ª•ng window (n·∫øu window>1, check consecutive)
        """
        # L·∫•y MSE trung b√¨nh m·ªói timestep
        detection = instance_errors > theta

        # If window exceeds one, look for consective detections
        if window > 1:

            detection = np.convolve(detection, np.ones(window), 'same') // window

            # clement: Removing this behavior
            # Backfill the windows (e.g. if idx 255 is 1, all of 255-window:255 should be filled)
            # fill_idxs = np.where(detection)
            # fill_detection = detection.copy()
            # for idx in fill_idxs[0]:
            #     fill_detection[idx - window : idx] = 1
            # return fill_detection

        return detection
    def save_detection_params(self, best_theta, best_window):
        self.best_theta = best_theta
        self.best_window = best_window
        print(f"Storing best parameters as: theta={best_theta}, window={best_window}")

    def best_cached_detect(self, instance_errors):
        return self.cached_detect(instance_errors, theta=self.best_theta, window=self.best_window)
# ------------------ Training & Evaluation ------------------
def train_model(
    model,
    train_loader,
    val_loader,
    device,
    epochs=20,
    lr=5e-5,
    weight_decay=1e-4,
    use_scheduler=False,
    early_stopping_patience=5
):
    """
    - weight_decay: L2 regularization (AdamW)
    - use_scheduler: d√πng ReduceLROnPlateau n·∫øu mu·ªën
    - early_stopping_patience: d·ª´ng s·ªõm n·∫øu val_loss ko c·∫£i thi·ªán
    """

    # D√πng AdamW thay v√¨ Adam (th∆∞·ªùng khuy·∫øn ngh·ªã cho transformer)
    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.MSELoss()
    model.to(device)

    train_losses, val_losses = [], []
    val_maes, val_r2s, val_mses = [], [], []

    best_val_loss = float('inf')
    no_improve_count = 0

    # Scheduler gi·∫£m lr khi val_loss kh√¥ng c·∫£i thi·ªán
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2, verbose=True
    ) if use_scheduler else None

    for epoch in range(epochs):
        # -------- TRAIN --------
        model.train()
        total_loss = 0
        for batch in train_loader:
            inputs  = batch['inputs'].to(device)
            targets = batch['targets'].to(device)

            outputs = model(inputs)
            loss    = model.compute_loss(outputs, targets, lambda_smooth=0.1)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ‚úÇÔ∏è
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # -------- VALIDATION --------
        model.eval()
        val_loss = 0
        predictions, true_targets = [], []

        with torch.no_grad():
            for batch in val_loader:
                inputs = batch['inputs'].to(device)
                targets = batch['targets'].to(device)
                outputs = model(inputs)

                loss = criterion(outputs, targets)
                val_loss += loss.item()

                predictions.append(outputs.cpu().numpy())
                true_targets.append(targets.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        preds = np.vstack(predictions)
        trues = np.vstack(true_targets)

        mse = mean_squared_error(trues, preds)
        mae = mean_absolute_error(trues, preds)
        r2 = r2_score(trues, preds)
        val_mses.append(mse)
        val_maes.append(mae)
        val_r2s.append(r2)

        print(
            f"Epoch {epoch+1}/{epochs}, "
            f"Train Loss: {avg_train_loss:.4f}, "
            f"Val Loss: {avg_val_loss:.4f}, "
            f"MSE: {mse:.4f}, MAE: {mae:.4f}, R¬≤: {r2:.4f}"
        )

        # Update scheduler
        if scheduler:
            scheduler.step(avg_val_loss)

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            no_improve_count = 0
        else:
            no_improve_count += 1
            if no_improve_count >= early_stopping_patience:
                print(f"‚èπ Early stopping at epoch {epoch+1}")
                break

    return train_losses, val_losses, val_mses, val_maes, val_r2s

def plot_mse_per_feature(mse_per_feature, save_path="mse_per_feature.pdf", feature_names=None):
    num_features = len(mse_per_feature)
    x = np.arange(num_features)

    plt.figure(figsize=(12, 6))
    plt.bar(x, mse_per_feature, color='skyblue')
    if feature_names:
        plt.xticks(x, feature_names, rotation=90)
    else:
        plt.xticks(x, [f"F{i}" for i in x], rotation=90)
    plt.ylabel("MSE")
    plt.title("Mean Squared Error per Feature")
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    print(f"‚úÖ MSE plot saved to {save_path}")

def evaluate_model(model, data_loader, device, save_dir="results", model_name="my_model", feature_names=None):
    model.eval()
    predictions, targets = [], []

    os.makedirs(save_dir, exist_ok=True)

    with torch.no_grad():
        for batch in data_loader:
            inputs = batch['inputs'].to(device)
            target = batch['targets'].to(device)
            output = model(inputs)
            predictions.append(output.cpu().numpy())
            targets.append(target.cpu().numpy())

    predictions = np.vstack(predictions)
    targets = np.vstack(targets)

    # √âp ki·ªÉu float ƒë·ªÉ tr√°nh l·ªói json dump
    mse_total = float(mean_squared_error(targets, predictions))
    mae_total = float(mean_absolute_error(targets, predictions))
    r2_total = float(r2_score(targets, predictions))

    mse_per_feature = [
        float(mean_squared_error(targets[:, i], predictions[:, i]))
        for i in range(targets.shape[1])
    ]
    mae_per_feature = [
        float(mean_absolute_error(targets[:, i], predictions[:, i]))
        for i in range(targets.shape[1])
    ]
    r2_per_feature  = [
        float(r2_score(targets[:, i], predictions[:, i]))
        for i in range(targets.shape[1])
    ]

    print(f"\nüîç Evaluation Summary:")
    print(f"Total MSE: {mse_total:.4f}, MAE: {mae_total:.4f}, R¬≤: {r2_total:.4f}")

    # Save metrics to JSON
    metrics = {
        "overall": {
            "mse": mse_total,
            "mae": mae_total,
            "r2": r2_total
        },
        "per_feature": {
            "mse": mse_per_feature,
            "mae": mae_per_feature,
            "r2": r2_per_feature
        }
    }

    metrics_path = os.path.join(save_dir, f"{model_name}_metrics.json")
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=4)
    print(f"‚úÖ Metrics saved to {metrics_path}")

    # Save model weights
    model_path = os.path.join(save_dir, f"{model_name}.pt")
    torch.save(model.state_dict(), model_path)
    print(f"‚úÖ Model weights saved to {model_path}")

    # Plot MSE bar chart
    mse_plot_path = os.path.join(save_dir, f"{model_name}_mse_barplot.pdf")
    plot_mse_per_feature(mse_per_feature, save_path=mse_plot_path, feature_names=feature_names)

    return predictions, targets, metrics

def count_trainable_params(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"‚úÖ Trainable parameters: {trainable:,} / {total:,} ({(trainable/total)*100:.2f}%)")



def hyperparameter_search(event_detector,config, model_type, Xval, Xtest, Ytest, dataset_name, val_idxs=None, test_split=0.7, run_name='results', verbose=1):

    # model_name= "Distil_Bert"
    model_name= "Final_Secbert"
    do_batches = False

    Ytest = Ytest.astype(int)
    Xtest_val, Xtest_test, Ytest_val, Ytest_test = utils.custom_train_test_split(dataset_name, Xtest, Ytest, test_size=test_split, shuffle=False)

   
    history = 100

        # Clip the prediction to match LSTM prediction window
    Ytest_test = Ytest_test[history + 1:]
    Ytest_val = Ytest_val[history + 1:]
    # Ytest_test = Ytest_test[history :]
    # Ytest_val = Ytest_val[history :]
    do_batches = True   

    ##### Cross Validation
    # if val_idxs is None:
    #     validation_errors = event_detector.reconstruction_errors(Xval)
    
    
    validation_errors = event_detector.reconstruction_errors_by_idxs(Xfull, val_idxs, history=history)

    

    test_errors = event_detector.reconstruction_errors(Xtest_val)
    test_instance_errors = test_errors.mean(axis=1)

    # Default to empty dict. Will still do F1 for window=1, ptile=95%
    grid_config = config.get('grid_search', dict())

    cutoffs = grid_config.get('percentile', [0.95])
    windows = grid_config.get('window', [1])
    print(grid_config)
    eval_metrics = grid_config.get('metrics', ['F1'])
    print(eval_metrics)

    firstPlotsError = True
    firstNpysError = True

    for metric in eval_metrics:

        # FPR is a negative metric (lower is better)
        negative_metric = (metric == 'false_positive_rate')

        # FPR is a negative metric (lower is better)
        if negative_metric:
            best_metric = 1
        else:
            best_metric = -1000

        best_percentile = 0
        best_window = 0
        metric_vals = np.zeros((len(cutoffs), len(windows)))
        print("metric: ", metric)
        metric_func = metrics.get("F1")
        print("metric_fun: ",metric_func )

        for percentile_idx in range(len(cutoffs)):

            percentile = cutoffs[percentile_idx]

            # set threshold as quantile of average reconstruction error
            theta = np.quantile(validation_errors.mean(axis = 1), percentile)

            for window_idx in range(len(windows)):

                window = windows[window_idx]

                #Yhat = event_detector.detect(Xtest, theta = theta, window = window, batches=True)
                Yhat = event_detector.cached_detect(test_instance_errors, theta = theta, window = window)
                Yhat = Yhat[window-1:].astype(int)

                Yhat, Ytest_val = utils.normalize_array_length(Yhat, Ytest_val)
                # choice_value = metric_func(Yhat, Ytest_val)
                # chinh sua lai su dung f1 skeatleatn
                choice_value = f1_score(Ytest_val, Yhat, average='micro')

                if verbose > 0:
                    print("{} is {:.3f} at theta={:.3f}, percentile={:.4f}, window={}".format(metric, choice_value, theta, percentile, window))

                # FPR is a negative metric (lower is better)
                if negative_metric:
                    if choice_value < best_metric:
                        best_metric = choice_value
                        best_percentile = percentile
                        best_window = window
                else:
                    if choice_value > best_metric:
                        best_metric = choice_value
                        best_percentile = percentile
                        best_window = window

                if grid_config.get('save-metric-info', False):
                    metric_vals[percentile_idx, window_idx] = choice_value

                if grid_config.get('detection-plots', False):

                    fig_detect, ax_detect = plt.subplots(figsize=(20, 4))

                    ax_detect.plot(Yhat, color = '0.1', label = 'predicted state')
                    ax_detect.plot(Ytest_val, color = 'r', alpha = 0.75, lw = 2, label = 'real state')
                    ax_detect.fill_between(np.arange(len(Yhat)), 0, Yhat.astype(int), color = '0.1')
                    ax_detect.set_title(
                        'Detection trajectory on test dataset, {}, percentile={:.3f}, window={}'
                        .format(model_type, percentile, window), fontsize = 14)
                    ax_detect.set_yticks([0,1])
                    ax_detect.set_yticklabels(['NO ATTACK','ATTACK'])
                    ax_detect.legend(fontsize = 12, loc = 2)
                    try:
                        fig_detect.savefig(f'plots/{run_name}/{model_name}-{percentile}-{window}.pdf')
                        if firstPlotsError:
                            print(f"Saving plots for model {model_name} to plots/{run_name}")
                            firstPlotsError = False
                    except FileNotFoundError:
                        fig_detect.savefig(f'plots/results/{model_name}-{percentile}-{window}.pdf')
                        if firstPlotsError:
                            print(f"Directory plots/{run_name}/ not found, saving plots for model {model_name} to plots/results/ instead")
                            firstPlotsError = False
                    plt.close(fig_detect)

            if grid_config.get('save-theta', False):
                try:
                    pickle.dump(theta, open(f'models/{run_name}/{model_name}-{percentile}-theta.pkl', 'wb'))
                    print(f'Saved theta to models/{run_name}/{model_name}-{percentile}-theta.pkl')
                except FileNotFoundError:
                    pickle.dump(theta, open(f'models/results/{model_name}-{percentile}-theta.pkl', 'wb'))
                    print(f"Directory models/{run_name}/ not found, saved theta to models/results/{model_name}-{percentile}-theta.pkl instead")

        print("Best metric ({}) is {:.3f} at percentile={:.5f}, window {}".format(metric, best_metric, best_percentile, best_window))

        # Final test performance
        final_test_errors = event_detector.reconstruction_errors(Xtest_test)
        final_test_instance_errors = final_test_errors.mean(axis=1)

        best_theta = np.quantile(validation_errors.mean(axis = 1), best_percentile)
        event_detector.save_detection_params(best_theta=best_theta, best_window=best_window)

        final_Yhat = event_detector.best_cached_detect(final_test_instance_errors)
        final_Yhat = final_Yhat[best_window-1:].astype(int)

        metric_func = metrics.get(metric)
        final_Yhat, Ytest_test = utils.normalize_array_length(final_Yhat, Ytest_test)
        final_value = metric_func(Ytest_test,final_Yhat)
        final_value =final_value = f1_score(Ytest_test,final_Yhat, average= "micro")
        print("Final {} is {:.3f} at percentile={:.5f}, window {}".format(metric, final_value, best_percentile, best_window))

        if grid_config.get('save-metric-info', False):
            try:
                np.save(f'npys/{run_name}/{model_name}-{metric}.npy', metric_vals)
                print(f'Saved metric at npys/{run_name}/{model_name}-{metric}.npy')
            except FileNotFoundError:
                np.save(f'npys/results/{model_name}-{metric}.npy', metric_vals)
                if firstPlotsError:
                    print(f"Directory npys/{run_name}/ not found, saved metric {model_name}-{metric}.npy to npys/results/ instead")
                    firstPlotsError = False
                    
        # ****************
        # luu de ve bieu do ROC
        np.save(f'results/{model_name}_y_true.npy', Ytest_test)
        np.save(f'results/{model_name}_y_score.npy', final_Yhat)
        f1_micro = f1_score(Ytest_test, final_Yhat, average="micro")
        f1_macro = f1_score(Ytest_test, final_Yhat, average="macro")
        precision_micro = precision_score(Ytest_test, final_Yhat, average="micro")
        precision_macro = precision_score(Ytest_test, final_Yhat, average="macro")
        recall_micro = recall_score(Ytest_test, final_Yhat, average="micro")
        recall_macro = recall_score(Ytest_test, final_Yhat, average="macro")

        # output f1 score
        report = classification_report(Ytest_test, final_Yhat, digits=4)
        # roc curve
        fpr, tpr, _ = roc_curve(Ytest_test, final_Yhat)
        # precision-recall curve
        precision, recall, _ = precision_recall_curve(Ytest_test, final_Yhat)
        cm = confusion_matrix(Ytest_test, final_Yhat)
        df_cm = pd.DataFrame(cm, index=["Actual 0", "Actual 1"], columns=["Predicted 0", "Predicted 1"])
        # output this to a file
        with open(f"results/{model_name}-classification_report.txt", "w") as f:
            # f.write(f"test_errors: {test_errors}\n")
            f.write(report+"\n")
            f.write(f"F1 Score (micro): {f1_micro}\n")
            f.write(f"F1 Score (macro): {f1_macro}\n")
            f.write(f"Precision (micro): {precision_micro}\n")
            f.write(f"Precision (macro): {precision_macro}\n")
            f.write(f"Recall (micro): {recall_micro}\n")
            f.write(f"Recall (macro): {recall_macro}\n")
            f.write(f"Final {metric}: {final_value}\n")
            f.write(f"Best {metric}: {best_metric}\n")
            f.write(f"Best percentile: {best_percentile}\n")
            f.write(f"Best window: {best_window}\n")
            f.write(f"Confusion matrix:\n")
            f.write(f"{df_cm}\n")
            f.write(f"ROC Curve:\n")
            f.write(f"False Positive Rate: {fpr}\n")
            f.write(f"True Positive Rate: {tpr}\n")
            f.write(f"Precision-Recall Curve:\n")
            f.write(f"Precision: {precision}\n")
            f.write(f"Recall: {recall}\n")
        print("Classification report ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'classification_report.txt'")
    return event_detector

# ------------------ MAIN ------------------
if __name__ == "__main__":
    import random
    #model_type = "Final_secBert"
    model_type= "SEC_Bert"
    dataset_name = "MiTM"
    history = 100
    test_ratio = 0.2

    config = {
        'grid_search': {
            # M·∫∑c ƒë·ªãnh mu·ªën qu√©t threshold ·ªü m·ª©c percentile=0.95
            'percentile': [ 
                0.90, 0.91,0.92, 0.93, 0.94, 
            0.95,
            0.96,
            0.97,
            0.98,
            0.99,
            0.991,
            0.992,
            0.993,
            0.994,
            0.995,
            0.996,
            0.997,
            0.998,
            0.999,
            0.9995,
            0.99995,], 

            # M·∫∑c ƒë·ªãnh ch·ªâ ki·ªÉm tra window=1 (kh√¥ng d√πng smoothing window)
            'window': [1, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],         

            # M·∫∑c ƒë·ªãnh d√πng metric F1
            'metrics': ['F1'],     

            'pr-plot': False,
            'detection-plots': False,
            'save-metric-info': True,
            'save-theta': True
        }
    }
   
    # 1) Load data
    import joblib
    data = joblib.load("X_full.pkl")
    Xfull = data["X_full"]
    columns = data["columns"]
    scaler = data["scaler"]
    Xtest, Ytest, df_test = load_and_preprocess_test("/home/ics-security/ICS-Detection/datasets/mitm_hmi_scapy_forward_1plc_6h_flow_label.csv")
    # Xfull = Xfull[:1000]
    print("Xshape: ", Xfull.shape)
    print("X[0]: ", Xfull[0])

    # -- (A) Chu·∫©n h√≥a X (khuy·∫øn ngh·ªã) --
   

    # 2) Chia random 80-20
    # X_train, X_val, _, _  = train_test_split(Xfull, Xfull, test_size=0.2, random_state=42, shuffle=False)
    train_idxs, val_idxs = utils.train_val_history_idx_split(Xfull, history)
    
    # print("After split, X_train:", X_train.shape, "X_val:", X_val.shape)

    # ## test code
    # X_train= X_train[0: 300] 
    
    # 3) T·∫°o chu·ªói (sequence) v√† target cho TRAIN
    train_seq = [Xfull[i - history:i] for i in train_idxs]
    train_tgt = [Xfull[i+1] for i in train_idxs]

    # 4) T·∫°o chu·ªói (sequence) v√† target cho VAL
    val_seq = [Xfull[i - history:i] for i in val_idxs]
    val_tgt = [Xfull[i+1] for i in val_idxs]

    print("train_seq len:", len(train_seq), "val_seq len:", len(val_seq))

    # 5) Dataset, DataLoader
    train_ds = TimeSeriesDataset(train_seq, train_tgt)
    val_ds   = TimeSeriesDataset(val_seq, val_tgt)
    train_loader = DataLoader(train_ds, batch_size=64,
                          shuffle=True, num_workers=4,
                          drop_last=False, pin_memory=True)
    val_loader   = DataLoader(val_ds, batch_size=64,
                            shuffle=False, num_workers=4,
                            drop_last=False, pin_memory=True)
    
    input_dim = len(columns)
    output_dim = len(columns)

    # 6) Kh·ªüi t·∫°o model (v·ªõi freeze layer)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TimeSeriesBERTFineTune(
        input_dim=input_dim,
        output_dim=output_dim,
        seq_len=history,
        freeze_until_layer=12  # Freeze encoder.layer.0 ƒë·∫øn encoder.layer.7
    )

    count_trainable_params(model)
    
  
    # model.load_state_dict(torch.load("eval_results/bert_sec_finetune.pt"))
    # model.to(device)
    # model.eval()


     

    # 7) Hu·∫•n luy·ªán (th·ª≠ weight_decay=1e-4, use_scheduler=True, early_stopping_patience=5)
    train_losses, val_losses, val_mses, val_maes, val_r2s = train_model(
        model,
        train_loader,
        val_loader,
        device,
        epochs=20,
        lr=5e-5,
        
        weight_decay=1e-4,
        use_scheduler=True,
        early_stopping_patience=5
    )
    
    # Ve do thi lose
    plt.figure(figsize=(8, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('results/loss_curve.pdf', dpi=300)  # ho·∫∑c .pdf ƒë·ªÉ ƒë∆∞a v√†o paper
    # plt.show()
    plt.close()
        
    loss_df = pd.DataFrame({
        'epoch': list(range(1, len(train_losses)+1)),
        'train_loss': train_losses,
        'val_loss': val_losses,
        'val_mse': val_mses,
        'val_mae': val_maes,
        'val_r2': val_r2s
    })

    loss_df.to_csv('results/training_logs.csv', index=False)
    ######################


    # 8) Evaluate + plot
    preds, trues, _ = evaluate_model(
        model,
        val_loader,
        device,
        save_dir="eval_results",
        model_name="SecBert",
        feature_names=None
    )
    validation_errors = model.reconstruction_errors_by_idxs(Xfull, val_idxs, history=history)
    val_errors =validation_errors.mean(axis=1)
    print("validation: ",validation_errors.mean(axis=1))
    with open(f"{model_type}- validation", "w") as f:
        f.write("validation per timestep:\n")
        for val in val_errors:
            f.write(f"{val}\n")

   
    
    hyperparameter_search(model,config, model_type, Xfull, Xtest, Ytest,dataset_name,val_idxs= val_idxs,verbose=0)